{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add word2vec to python path.\n",
    "import sys\n",
    "sys.path.append(\"external/word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Config\n",
    "config = {\n",
    "    \"use_cuda\": True,\n",
    "    \n",
    "    \"data_dir\": \"data/\",\n",
    "    \"data_size\": -1,\n",
    "    \"pre_trained_vocab_path\": \"models/skipgram/vocab.pt\",\n",
    "    \"pre_train_embedding_path\": \"models/skipgram/best_val_model_4.27.pt\",\n",
    "    \n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"num_epochs\": 64,\n",
    "    \n",
    "    \"model_dir\": \"models/title-predictor/\",\n",
    "    \"checkpoint_freq\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# Data Processing.\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# Word2Vec.\n",
    "from external.word2vec.utils.helper import (\n",
    "    load_vocab,\n",
    ")\n",
    "\n",
    "# Title Predictor.\n",
    "from title_predictor_model import TitlePredictor\n",
    "\n",
    "# Plotting.\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"Solarize_Light2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability.\n",
    "if config[\"use_cuda\"]:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model directory exists.\n",
    "if not os.path.exists(config[\"model_dir\"]):\n",
    "    os.makedirs(config[\"model_dir\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoDf = pd.read_csv(\"data/usvideos_rm_dup.csv\")\n",
    "# Remove all nan view values.\n",
    "videoDf = videoDf.dropna()\n",
    "videoDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for twitter financial news text.\n",
    "class YouTubeTitleViewDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, size = -1):\n",
    "        self.titleDf = df\n",
    "        # Shuffle and take a subset of the data.\n",
    "        if size > 0:\n",
    "            self.titleDf = self.titleDf.sample(frac=1).reset_index(drop=True)\n",
    "            self.titleDf = self.titleDf[:size]\n",
    "        else:\n",
    "            self.titleDf = self.titleDf.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.titleDf)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        title = self.titleDf[\"title\"][idx]\n",
    "        view = self.titleDf[\"views\"][idx]\n",
    "        return str(title), view\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create title dataset.\n",
    "titleDataset = YouTubeTitleViewDataset(videoDf, size = config[\"data_size\"])\n",
    "len(titleDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test dataset.\n",
    "trainSet, valSet = train_test_split(titleDataset, test_size=0.2)\n",
    "len(trainSet), len(valSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titleDataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Vocab and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (config[\"pre_trained_vocab_path\"]):\n",
    "    vocab: Vocab = load_vocab(config[\"pre_trained_vocab_path\"])\n",
    "    vocabSize = len(vocab.get_stoi())\n",
    "    print(f\"Pretrained vocab size: {vocabSize}\")\n",
    "else:\n",
    "    print(f\"No vocab path provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SOS and EOS tokens.\n",
    "vocab.append_token(\"<SOS>\")\n",
    "vocab.append_token(\"<EOS>\")\n",
    "vocabSize = len(vocab.get_stoi())\n",
    "print(f\"Vocab size: {vocabSize}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the english tokenizer.\n",
    "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first sentence and view.\n",
    "sentence, view = titleDataset[0]\n",
    "print(sentence)\n",
    "print(view)\n",
    "# Tokenize the sentence.\n",
    "tokenizedSentence: list = tokenizer(sentence)\n",
    "tokenizedSentence.insert(0, \"<SOS>\")\n",
    "tokenizedSentence.append(\"<EOS>\")\n",
    "print(tokenizedSentence)\n",
    "# Convert the tokens to ids.\n",
    "print(vocab(tokenizedSentence))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pretrained embedding weight.\n",
    "embeddingWeight = torch.load(config[\"pre_train_embedding_path\"])[\"embeddings.weight\"]\n",
    "print(f\"Embedding weight shape: {embeddingWeight.shape}\")\n",
    "# Create an embedding layer.\n",
    "embedding = nn.Embedding(vocabSize, embeddingWeight.shape[1])\n",
    "print(f\"Embedding layer shape: {embedding.weight.shape}\")\n",
    "# Load pretrained embedding weight.\n",
    "with torch.no_grad():\n",
    "    embedding.weight.data[:embeddingWeight.shape[0]] = embeddingWeight\n",
    "print(f\"Pretrained embedding loaded. Embedding layer shape: {embedding.weight.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model.\n",
    "model = TitlePredictor(\n",
    "    vocabSize,\n",
    "    embeddingSize = embeddingWeight.shape[1],\n",
    "    hiddenSize = 256,\n",
    "    numLayers = 2,\n",
    "    dropout = 0.5,\n",
    "    embedding = embedding\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss function and optimizer.\n",
    "lossFunction = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLosses = []\n",
    "valLosses = []\n",
    "startTime = time.time()\n",
    "bestValLoss = float(\"inf\")\n",
    "bestValModel = None\n",
    "\n",
    "# Train the model.\n",
    "for epoch in range(config[\"num_epochs\"]):\n",
    "    print(f\"Epoch: {epoch+1}/{config['num_epochs']}\")\n",
    "    # Train the model.\n",
    "    trainLossVal = 0\n",
    "    valLossVal = 0\n",
    "    for i, (title, view) in enumerate(trainSet):\n",
    "        # Tokenize the sentence.\n",
    "        tokenizedSentence: list = tokenizer(title)\n",
    "        tokenizedSentence.insert(0, \"<SOS>\")\n",
    "        tokenizedSentence.append(\"<EOS>\")\n",
    "        # Convert the tokens to ids.\n",
    "        tokenizedSentence = vocab(tokenizedSentence)\n",
    "        # Convert to tensor.\n",
    "        tokenizedSentence = torch.tensor(tokenizedSentence).to(device)\n",
    "        # Reshape to [sentence length, 1]\n",
    "        tokenizedSentence = tokenizedSentence.unsqueeze(1)\n",
    "        # Get the view.\n",
    "        view = torch.tensor([view]).to(device)\n",
    "        # Get the natural log of the view.\n",
    "        view = torch.log(view)\n",
    "        # Reshape to [1]\n",
    "        view = view.unsqueeze(0)\n",
    "        # Forward pass.\n",
    "        prediction = model(tokenizedSentence)\n",
    "        # Compute loss.\n",
    "        trainLoss = lossFunction(prediction, view.float())\n",
    "        # Backward pass.\n",
    "        optimizer.zero_grad()\n",
    "        trainLoss.backward()\n",
    "        # Update parameters.\n",
    "        optimizer.step()\n",
    "        # Add loss to train loss.\n",
    "        trainLossVal += trainLoss.item()\n",
    "        \n",
    "        print(f\"Sentence: {i+1}/{len(trainSet)}\", end=\"\\r\")\n",
    "    \n",
    "    # Validate the model.\n",
    "    with torch.no_grad():\n",
    "        for i, (title, view) in enumerate(valSet):\n",
    "            # Tokenize the sentence.\n",
    "            tokenizedSentence: list = tokenizer(title)\n",
    "            tokenizedSentence.insert(0, \"<SOS>\")\n",
    "            tokenizedSentence.append(\"<EOS>\")\n",
    "            # Convert the tokens to ids.\n",
    "            tokenizedSentence = vocab(tokenizedSentence)\n",
    "            # Convert to tensor.\n",
    "            tokenizedSentence = torch.tensor(tokenizedSentence).to(device)\n",
    "            # Reshape to [sentence length, 1]\n",
    "            tokenizedSentence = tokenizedSentence.unsqueeze(1)\n",
    "            # Get the view.\n",
    "            view = torch.tensor([view]).to(device)\n",
    "            # Get the natural log of the view.\n",
    "            view = torch.log(view)\n",
    "            # reshape to [1]\n",
    "            view = view.unsqueeze(0)\n",
    "            # Forward pass.\n",
    "            prediction = model(tokenizedSentence)\n",
    "            # Compute loss.\n",
    "            valLoss = lossFunction(prediction, view)\n",
    "            # Add loss to validation loss.\n",
    "            valLossVal += valLoss.item()\n",
    "    \n",
    "    avgTrainLoss = trainLossVal/len(trainSet)\n",
    "    avgValLoss = valLossVal/len(valSet)\n",
    "    \n",
    "    # Print loss.\n",
    "    print(f\"Epoch: {epoch+1}/{config['num_epochs']}, Train Loss: {avgTrainLoss:.4f}, Val Loss: {avgValLoss:.4f}\")\n",
    "    \n",
    "    trainLosses.append(avgTrainLoss)\n",
    "    valLosses.append(avgValLoss)\n",
    "    \n",
    "    # Save the model with the lowest validation loss.\n",
    "    if avgValLoss < bestValLoss:\n",
    "        bestValLoss = avgValLoss\n",
    "        bestValModel = model\n",
    "        print(\"Best model saved.\")\n",
    "    \n",
    "    # Save checkpoint.\n",
    "    if (epoch+1) % config[\"checkpoint_freq\"] == 0:\n",
    "        checkpoint = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"vocabSize\": model.vocabSize,\n",
    "            \"embeddingSize\": model.embeddingSize,\n",
    "            \"hiddenSize\": model.hiddenSize,\n",
    "            \"numLayers\": model.numLayers,\n",
    "            \"dropout\": model.dropoutPr,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"{config['model_dir']}/checkpoint_{epoch+1}.pt\")\n",
    "        print(f\"Checkpoint saved at epoch {epoch+1}.\")\n",
    "    \n",
    "    # Calculate time.\n",
    "    currTime = time.time()\n",
    "    elapsedTime = currTime - startTime\n",
    "    print(f\"Elapsed time: {elapsedTime/60:.2f} minutes.\")\n",
    "    print(f\"Average time per epoch: {elapsedTime/(epoch+1)/60:.2f} minutes.\")\n",
    "    print(f\"Estimated time remaining: {(config['num_epochs'] - epoch - 1)*elapsedTime/(epoch+1)/60:.2f} minutes.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model.\n",
    "torch.save({\n",
    "    \"model\": bestValModel.state_dict(),\n",
    "    \"vocabSize\": bestValModel.vocabSize,\n",
    "    \"embeddingSize\": bestValModel.embeddingSize,\n",
    "    \"hiddenSize\": bestValModel.hiddenSize,\n",
    "    \"numLayers\": bestValModel.numLayers,\n",
    "    \"dropout\": bestValModel.dropoutPr,\n",
    "}, f\"{config['model_dir']}/best_val_{bestValLoss:.2f}.pt\")\n",
    "# Save vocabulary.\n",
    "torch.save(vocab, f\"{config['model_dir']}/vocab.pt\")\n",
    "# Save the loss.\n",
    "with open(f\"{config['model_dir']}/loss.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"trainLosses\": trainLosses,\n",
    "        \"valLosses\": valLosses\n",
    "    }, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss.\n",
    "plt.plot(trainLosses, label=\"Train Loss\")\n",
    "plt.plot(valLosses, label=\"Val Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by view.\n",
    "videoDf = videoDf.sort_values(by=\"views\", ascending=True)\n",
    "videoDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 100 evenly spaced indices.\n",
    "indices = range(0, len(videoDf), len(videoDf)//100)\n",
    "indices = list(indices)\n",
    "# Get the title and view.\n",
    "titles = videoDf[\"title\"].iloc[indices].tolist()\n",
    "views = videoDf[\"views\"].iloc[indices].tolist()\n",
    "len(titles), len(views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the views.\n",
    "predictedViews = []\n",
    "with torch.no_grad():\n",
    "    for i, title in enumerate(titles):\n",
    "        # Tokenize the sentence.\n",
    "        tokenizedSentence: list = tokenizer(title)\n",
    "        tokenizedSentence.insert(0, \"<SOS>\")\n",
    "        tokenizedSentence.append(\"<EOS>\")\n",
    "        # Convert the tokens to ids.\n",
    "        tokenizedSentence = vocab(tokenizedSentence)\n",
    "        tokenizedSentence = torch.tensor(tokenizedSentence).to(device)\n",
    "        # Reshape to [sentence length, 1]\n",
    "        tokenizedSentence = tokenizedSentence.unsqueeze(1)\n",
    "        # Convert to tensor.\n",
    "        tokenizedSentence = torch.tensor(tokenizedSentence).to(device)\n",
    "        # Forward pass.\n",
    "        prediction = model(tokenizedSentence)\n",
    "        # Get the view.\n",
    "        predictedView = torch.exp(prediction).item()\n",
    "        predictedViews.append(predictedView)\n",
    "        print(f\"Sentence: {i+1}/{len(titles)}\", end=\"\\r\")\n",
    "len(predictedViews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[20], views[20], predictedViews[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert views and predicted views to log scale.\n",
    "logVies = np.log(views)\n",
    "logPredictedViews = np.log(predictedViews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted views and actual views.\n",
    "plt, ax = plt.subplots()\n",
    "ax.plot(logVies, label=\"Actual Views\")\n",
    "ax.plot(logPredictedViews, label=\"Predicted Views\")\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f92cc221f18ca953eb7b72a3bd486009e4f771bfc246436a572e9938561415e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
